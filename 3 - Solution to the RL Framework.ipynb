{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the RL Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the solution as a series of actions to be taken by the agent. But the correct action will change according to the current situation, so in different situation a different set of actions may be taken.\n",
    "\n",
    "Reward is always decided in the context of the state that it was decided, along with the state that follows.\n",
    "\n",
    "So, if the agent leanrs the appropriate response to any environment state (that is, if the agent knows what actions to take according the the different situations it faces), then it can be said that we have a solution to our problem. This motivates the idea of a policy.\n",
    "\n",
    "A policy can be thought of as a maping from the set of environment states to the set of possible actions.\n",
    "\n",
    "$$\\pi : \\mathbb{S} \\longrightarrow \\mathbb{A}$$\n",
    "\n",
    "We can think of this as:\n",
    "\n",
    "state (as input) $\\longrightarrow$ factory ($\\pi$) $\\longrightarrow$ action (as an output)\n",
    "\n",
    "Policies can be *deterministic* or *stochastic*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deterministic Policy - definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **deterministic policy** is a mapping $\\pi : \\mathbb{S} \\longrightarrow \\mathbb{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Policy - definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **stochastic policy** is a mapping $\\pi : \\mathbb{S} x \\mathbb{A} \\longrightarrow [0, 1]$.$$\\pi(a \\mid s) = \\mathbb{P}(A_t = a \\mid S_t = s)$$\n",
    "\n",
    "state (as input) $\\longrightarrow$ factory ($\\pi$) $\\longrightarrow$ probability that the agent takes action $a$ while in state $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a Deterministic Policy:\n",
    "\n",
    "$\\pi(low) = recharge$\n",
    "\n",
    "$\\pi(high) = search$\n",
    "\n",
    "#### Example of a Stochastic Policy:\n",
    "\n",
    "$\\pi(recharge \\mid low) = 0.5$\n",
    "\n",
    "$\\pi(wait \\mid low) = 0.4$\n",
    "\n",
    "$\\pi(search \\mid low) = 0.1$\n",
    "\n",
    "$\\pi(search \\mid high) = 0.9$\n",
    "\n",
    "$\\pi(wait \\mid high) = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value Function - formal definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For each state $s$, it yields the expected discounted return if the agent starts in state $s$ and then uses the policy to choose its actions for all time steps.\"\n",
    "\n",
    "The state value function will always correspond to a particular policy $\\pi$. So if we change the policy, then we change the state value functions as well. The state value function is tipically denoted as $v_{\\pi}$.\n",
    "\n",
    "The value of state $s$ under a policy $\\pi$ is:\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a state $s_t$ any state) is normally expressed as the sumatory of the immediate reward plus the value of the state that follows (i.e. $s_{t+1}$). So we can say that the value of a state is simply the *expected discounted reward* - the state values are/may be affected by a discount rate.\n",
    "\n",
    "So, to be more precise we can say that **the value of any state can be expressed as the sum of the immediate reward plus the *discounted value* of the state that follows**.\n",
    "\n",
    "This is expressed with the *Bellman Expectation Equation*:\n",
    "\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s]$$\n",
    "\n",
    "The value of any state $s$ is equal to the expected value of the immediate reward plus the discounted value of the state that follows, if the agent starts in state $S$ and then uses the policy $\\pi$ to choose its actions for tall the time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A policy $\\pi'$ is better or equal to the policy $\\pi$ if its state value function is greater than or equal to the policy $\\pi$ for all states.\n",
    "\n",
    "Points for consideration:\n",
    " * if we have any 2 policies we might not be able to decide which of them is better - it could be that the 2 policies are not comparable\n",
    " * it will always exist one policy which will be better than or equal to other policies - this would be the **optimal policy**.\n",
    " \n",
    "Formal definition:\n",
    "\n",
    "$\\pi' \\geq \\pi$ *if and only if* $v_{\\pi'}(s) \\geq v_{\\pi}(s)$ for all $s \\in S$\n",
    "\n",
    "An optimal policy $\\pi_*$ satisfies $\\pi_* \\geq \\pi$ for all $\\pi$\n",
    "\n",
    "*Hint*: An optimal policy is guaranteed to exist, but may not be unique\n",
    "\n",
    "*Hint*: The optimal state-value function is denoted $v_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do we determine an optimal policy for a complicated MDP?*\n",
    "\n",
    "We call $q_\\pi$ the **action-value function for policy $\\pi$**.\n",
    "\n",
    "The value of taking action $a$ in state $s$ under a policy $\\pi$ is:\n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "In words: For each state $s$ and action $a$, it yields the expected return if the agent starts in state $s$, then chooses action $a$ and then uses the policy $\\pi$ to choose its actions for all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deterministic policy $\\pi$,\n",
    "$$v_\\pi(s) = q_\\pi(s, \\pi(s))$$ holds for all $s \\in S$?\n",
    "\n",
    "**True**\n",
    "\n",
    "It depends on how we have defined the action-value function. The value of the state-action pair $s, \\pi(s)$ is the expected return if the agent starts in state $s$, takes action $\\pi(s)$ and then follows the policy $\\pi$. In other words, it is the expected return if the agent starts in state $s$ and then follows the policy $\\pi$, which is exactly equal to the value of state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy may be estimated by the agent $\\longrightarrow$ The agent interacts with the environment and from this interacton, it estimates the optimal action value function. Then the agent uses that value function to get the optimal policy.\n",
    "\n",
    "Interaciont $\\longrightarrow q_* \\longrightarrow \\pi_*$ \n",
    "\n",
    "Assuming the agent has already the action-value function and that it still does not have the optimal policy, then for each state, the agent picks thea ction that yields the highest expected return.\n",
    "\n",
    "If the state space $S$ and action space $A$ are finite, we can represent the optimal caction-value function $q_*$ in a table, where we have one entry for each entry for each possible environment state $s \\in S$ and action $a \\in A$.\n",
    "The value for a particular state-action pair $s, a$ is the expected return if the agent starts in state $s$, takes action $a$, and then follows the optimal policy $\\pi_*$.\n",
    "\n",
    "The table below is populated with some values for a hypothetical Markov decision process - where $\\mathbb{S} = \\{s_1, s_2, s_3\\}$ and $\\mathbb{A} = \\{a_1, a_2, a_3\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| _ |$a_1$ | $a_2$ | $a_3$ |\n",
    "|------|------|------|------|\n",
    "| $s_1$  | 1 | 2 | -3|\n",
    "| $s_2$  | -2 | 1 | 3|\n",
    "| $s_3$  | 4 | 4 | -5|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an agent has determined the optimal aciton-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting:\n",
    "\n",
    "$$\\pi_*(s) = arg max_{a \\in A(s)}q_*(s, a)$$\n",
    "for all $s \\in S$.\n",
    "\n",
    "Note, that for this to be true, it must hold also true that:\n",
    "\n",
    "$$v_*(s) = max_{a \\in A(s)}q_*(s, a)$$\n",
    "\n",
    "In the event that there is some state $s \\in S$ for which multiple actions $a \\in A(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the maximizing actions. It must be only ensure that the actions that do not maximize the action-value function for a particular state are given 0% probability under the policy.\n",
    "\n",
    "Therefore, the optimal policy $\\pi_*$ for the corresponding MDP must satisfy:\n",
    " * $\\pi_*(s_1) = a_2$, or equivalently, $\\pi_*(a_2 \\mid s_1) = 1$\n",
    " * $\\pi_*(s_2) = a_3$, or equivalently, $\\pi_*(a_3 \\mid s_2) = 1$\n",
    " \n",
    "Why? Because:\n",
    " * $a_2 = arg max_{a \\in A(s_1)}q_*(s_1, a)$, and\n",
    " * $a_3 = arg max_{a \\in A(s_2)}q_*(s_2, a)$\n",
    " \n",
    " \n",
    " In other words, under the optimal policy, the agent must choose action $a_2$ when in state $s_1$, and it must choose action $a_3$ when in state $s_2$.\n",
    " \n",
    " Note that for state $s_3$, $a_1, a_2 \\in argmax_{a \\in A(s_3)}q_*(s_3, a)$ and therefore the agent can choose either action $a_1$ or $a_2$ under the optimal policy, but it can never choose acton $a_3$. So, the optimal policy $\\pi_*$ must satisfy:\n",
    "  * $\\pi_*(a_1 \\mid s_3) = p$,\n",
    "  * $\\pi_*(a_2 \\mid s_3) = q$,\n",
    "  * $\\pi_*(a_3 \\mid s_3) = 0$\n",
    "  \n",
    "where $p, q \\geq 0$, and $p + q = 1$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
