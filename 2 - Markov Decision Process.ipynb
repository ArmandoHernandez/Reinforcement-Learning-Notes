{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an introduction to the MDPs, let's start with an example (which I took from the [Machine Learning Nanodegree from Udacity](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t)).\n",
    "\n",
    "Let's suppose that we have a roboto that collects empty cans of soda without any human intervention and it should decide by itself whether or not he needs to recharge its batteries - which means it has to go to the charging area. We want to define this problem by means of the *MDP*.\n",
    "\n",
    "So, **what are the actions?**\n",
    "\n",
    "    * search for a can\n",
    "    * recharge its battery\n",
    "    * wait for a can to be collected\n",
    "    \n",
    "We call the set of all possible actions that an agent can take within the context of this problem (that is, the available actions to the agent), the **Action Space**.\n",
    "\n",
    "So, **what are the states?**\n",
    "\n",
    "    * battery high\n",
    "    * battery low\n",
    "    \n",
    "We call the set of all non-terminal states, the **State Space**. $S^+$ denotes the state space, including terminal states.\n",
    "\n",
    "In case there are some states where only a subset of the actions are available, we can also use $A(s)$ to refer to the set of actions available in state $s \\in S$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a diagram that shows the state diagram with probability of a transition to occur (blue numbers) and the reward (in number of empty soda cans) that transition brings (orange numbers).\n",
    "\n",
    "![MDP diagram](images/markovDiagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision Process (MDP); the definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (finite) Markov Decision Process (MDP) is defined by:\n",
    "\n",
    "   * a (finite) set of states $S$\n",
    "   * a (finite) set of actions $A$\n",
    "   * a (finite) set of rewards $W$\n",
    "   * the one-step dynamics of the environment:\n",
    "   $$p(s', r \\mid s,a) = \\mathbb{P}(S_{t+1} = s', R_{t+1} = r \\mid S_t = s, A_t = a)$$\n",
    "   for all s, s', a and r\n",
    "   * a discount rate $\\gamma \\in [0, 1]$\n",
    "   \n",
    "*Hint:* The discount rate will have to be different than 0, but maybe close to 1 to avoid that the agent becomes too short-sighted to a fault. An example to remember discount rate; let us define a discount rate as $\\gamma = 0.9$. Our discounted return would then be:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ...$$ and continues without limit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
